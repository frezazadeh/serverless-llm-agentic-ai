{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0566ad22",
      "metadata": {
        "id": "0566ad22"
      },
      "source": [
        "# Serverless LLMs and Agentic AI with Modal â€“ Lesson 2\n",
        "## Scaling & Input Concurrency\n",
        "\n",
        "In this lesson, youâ€™ll explore **how Modal scales your code** and how you can **tune that behavior**.\n",
        "\n",
        "You will:\n",
        "\n",
        "- Use a *new* demo function that imitates a small \"API-style\" workload (not CPU heavy).\n",
        "- See how Modal automatically scales containers when you submit many tasks.\n",
        "- Experiment with:\n",
        "  - `max_containers`, `min_containers`, `scaledown_window`\n",
        "  - `@modal.concurrent(max_inputs=..., target_inputs=...)`\n",
        "- Compare behavior **with and without** input concurrency.\n",
        "- Inspect container activity in the **Modal dashboard**.\n",
        "\n",
        "> This notebook assumes you already know how to authenticate with Modal from Lesson 1.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85daf2ca",
      "metadata": {
        "id": "85daf2ca"
      },
      "outputs": [],
      "source": [
        "# =====================================\n",
        "# Step 0 â€“ Install and check Modal\n",
        "# =====================================\n",
        "# Run this cell once at the start of your Colab session.\n",
        "\n",
        "!pip install modal --quiet\n",
        "\n",
        "# Show where the `modal` CLI is and which version is installed\n",
        "!which modal\n",
        "!modal --version\n",
        "\n",
        "print(\"âœ… Modal installed and detected above (path + version).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0aa4d2e",
      "metadata": {
        "id": "c0aa4d2e"
      },
      "source": [
        "## Step 1 â€“ Make sure you are authenticated\n",
        "\n",
        "Before you can run this notebook successfully, your environment must be authenticated with Modal.\n",
        "\n",
        "If you are in **Colab** and didnâ€™t configure a token yet, do this **in a terminal or local shell**:\n",
        "\n",
        "```bash\n",
        "modal token set --token-id <YOUR_TOKEN_ID> --token-secret <YOUR_TOKEN_SECRET>\n",
        "modal token -h\n",
        "```\n",
        "\n",
        "If you already did this in a previous lesson on the same machine, youâ€™re good to go.\n",
        "\n",
        "In this notebook, weâ€™ll just *verify* that Modal can talk to the server."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!modal token set --token-id <YOUR_TOKEN_ID> --token-secret <YOUR_TOKEN_SECRET>\n",
        "!modal token -h || echo 'Auth error - run \"modal token set\" in a terminal and retry.'"
      ],
      "metadata": {
        "id": "MZ4QFWEJtfeJ"
      },
      "id": "MZ4QFWEJtfeJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "e2600587",
      "metadata": {
        "id": "e2600587"
      },
      "source": [
        "## Step 2 â€“ A New Demo Scenario: Simulated â€œAPI-styleâ€ Requests\n",
        "\n",
        "Weâ€™ll use a **new function** that acts like a small API call:\n",
        "\n",
        "- It pretends to:\n",
        "  - receive a request (with an ID and payload size)\n",
        "  - spend some time â€œwaitingâ€ (simulating network / IO)\n",
        "  - lightly touch the CPU (e.g., a short checksum-like loop)\n",
        "- It returns a tiny response dictionary with timing info.\n",
        "\n",
        "Weâ€™ll run this function many times in parallel to see how Modal scales.\n",
        "\n",
        "Youâ€™ll be able to tweak:\n",
        "- the **sleep time** to simulate slower/faster requests\n",
        "- the **number of requests** (e.g. 20 vs 500 vs 2000)\n",
        "- the **scaling and concurrency settings**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f91dace",
      "metadata": {
        "id": "2f91dace"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import random\n",
        "from typing import Dict\n",
        "\n",
        "import modal\n",
        "\n",
        "# Create a dedicated app for this lesson\n",
        "app = modal.App(\"lesson2-scaling-and-concurrency\")\n",
        "\n",
        "\n",
        "def simulate_api_request(request_id: int, payload_kb: int, base_latency: float = 0.3) -> Dict:\n",
        "    \"\"\"Simulate handling a small 'API-style' request.\n",
        "\n",
        "    This is NOT CPU heavy. It intentionally looks like:\n",
        "      - A bit of network / IO waiting (time.sleep)\n",
        "      - A tiny amount of CPU work (a short loop)\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    request_id : int\n",
        "        Just an identifier so we can track which call this was.\n",
        "    payload_kb : int\n",
        "        A fake \"payload size\" in kilobytes, used to slightly vary the work.\n",
        "    base_latency : float\n",
        "        Minimum time (in seconds) that each request takes.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Dict with:\n",
        "      - request_id\n",
        "      - payload_kb\n",
        "      - simulated_latency\n",
        "      - pseudo_checksum\n",
        "    \"\"\"\n",
        "    # Simulate variable latency: base time plus up to 200 ms jitter\n",
        "    jitter = random.uniform(0.0, 0.2)\n",
        "    simulated_latency = base_latency + jitter\n",
        "\n",
        "    # Sleep to mimic waiting on a database / external API\n",
        "    time.sleep(simulated_latency)\n",
        "\n",
        "    # Light CPU work: a tiny checksum based on payload size\n",
        "    # (This is intentionally small so the example stays IO-ish.)\n",
        "    pseudo_checksum = 0\n",
        "    for i in range(payload_kb * 10):\n",
        "        pseudo_checksum = (pseudo_checksum + i) % 97\n",
        "\n",
        "    return {\n",
        "        \"request_id\": request_id,\n",
        "        \"payload_kb\": payload_kb,\n",
        "        \"simulated_latency\": round(simulated_latency, 3),\n",
        "        \"checksum\": pseudo_checksum,\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "875522b3",
      "metadata": {
        "id": "875522b3"
      },
      "source": [
        "## Step 3 â€“ Auto-scaling configuration for our function\n",
        "\n",
        "Modal will **auto-scale** by default, but we can tune behavior by adding parameters to `@app.function`:\n",
        "\n",
        "- `max_containers` â€“ upper limit on containers for this function\n",
        "- `min_containers` â€“ keep a minimum number of warm containers around\n",
        "- `scaledown_window` â€“ how long idle containers should stay alive before being shut down\n",
        "\n",
        "Weâ€™ll create a function `handle_request_scaled` where you can play with these settings.\n",
        "\n",
        "> ðŸ§ª **Experiment:** Run with different values and watch the Modal dashboard.\n",
        "> For example:\n",
        "> - `max_containers=2`  vs  `max_containers=20`\n",
        "> - `min_containers=0`  vs  `min_containers=5`\n",
        "> - `scaledown_window=None`  vs  `scaledown_window=45`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "853be060",
      "metadata": {
        "id": "853be060"
      },
      "outputs": [],
      "source": [
        "# ==================================================\n",
        "# Function 1 â€“ Focus on container scaling behavior\n",
        "# ==================================================\n",
        "#\n",
        "# Start with everything = None to let Modal decide.\n",
        "# Then try setting your own values and compare.\n",
        "#\n",
        "\n",
        "@app.function(\n",
        "    max_containers=None,      # e.g. 2, 5, 20\n",
        "    min_containers=None,      # e.g. 2 or 5\n",
        "    scaledown_window=None,    # e.g. 30 or 60 seconds\n",
        ")\n",
        "def handle_request_scaled(request_id: int, payload_kb: int) -> Dict:\n",
        "    \"\"\"Handle a single 'API request' with configurable scaling.\n",
        "\n",
        "    This function is identical in logic to `simulate_api_request`,\n",
        "    but runs inside a Modal container and uses scaling settings\n",
        "    from the decorator above.\n",
        "    \"\"\"\n",
        "    return simulate_api_request(request_id=request_id, payload_kb=payload_kb)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94fddf51",
      "metadata": {
        "id": "94fddf51"
      },
      "source": [
        "## Step 4 â€“ Input concurrency (many inputs per container)\n",
        "\n",
        "By default, each Modal container processes **one input at a time**.\n",
        "\n",
        "For IO-bound workloads like our simulated API call, we might prefer:\n",
        "\n",
        "> â€œFewer containers, each handling many requests in parallel.â€\n",
        "\n",
        "This is where `@modal.concurrent` comes in:\n",
        "\n",
        "- `max_inputs` â€“ maximum number of in-flight calls per container\n",
        "- `target_inputs` â€“ soft target concurrency level (autoscaler tries to hover around this)\n",
        "\n",
        "Weâ€™ll make another function `handle_request_concurrent` to explore this.\n",
        "\n",
        "> âš ï¸ Important: `@modal.concurrent` must come **below** `@app.function` in the decorator stack."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3fe1232",
      "metadata": {
        "id": "c3fe1232"
      },
      "outputs": [],
      "source": [
        "# ==================================================\n",
        "# Function 2 â€“ Enable input concurrency per container\n",
        "# ==================================================\n",
        "#\n",
        "# Try combinations like:\n",
        "#   max_inputs=50, target_inputs=40\n",
        "#   max_inputs=100, target_inputs=80\n",
        "#\n",
        "# Start with small values to see the effect more clearly.\n",
        "#\n",
        "\n",
        "@app.function()\n",
        "@modal.concurrent(\n",
        "    max_inputs=50,   # REQUIRED: try 20, 50, 100, 200\n",
        "    target_inputs=40 # OPTIONAL: soft target near max_inputs\n",
        ")\n",
        "def handle_request_concurrent(request_id: int, payload_kb: int) -> Dict:\n",
        "\n",
        "    \"\"\"Handle a single 'API request' with input concurrency enabled.\n",
        "\n",
        "    With `@modal.concurrent`, a single container can process many requests\n",
        "    at the same time, which is ideal for IO-bound workloads like this one.\n",
        "    \"\"\"\n",
        "    return simulate_api_request(request_id=request_id, payload_kb=payload_kb, base_latency=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "307b9465",
      "metadata": {
        "id": "307b9465"
      },
      "source": [
        "## Step 5 â€“ Main entrypoint: batch of requests\n",
        "\n",
        "Our entrypoint will:\n",
        "\n",
        "1. Generate a list of â€œrequestsâ€\n",
        "   - request IDs: 0, 1, 2, ..., `num_requests - 1`\n",
        "   - payload sizes: random small values (e.g. 10â€“80 KB)\n",
        "2. Decide whether to use:\n",
        "   - `handle_request_scaled` (focus on container scaling)\n",
        "   - `handle_request_concurrent` (focus on input concurrency)\n",
        "3. Use `.remote(...)` in a loop to submit all requests to Modal.\n",
        "4. Measure how long everything took.\n",
        "\n",
        "> For Colab, you typically copy this notebook into a `.py` file and run it with Modal:\n",
        ">\n",
        "> ```bash\n",
        "> modal run lesson2_scaling.py --num-requests 200\n",
        "> ```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f50bff2",
      "metadata": {
        "id": "2f50bff2"
      },
      "outputs": [],
      "source": [
        "@app.local_entrypoint()\n",
        "def main(\n",
        "    use_concurrency: bool = False,\n",
        "    num_requests: int = 100,\n",
        "):\n",
        "    \"\"\"Lesson 2 entrypoint.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    use_concurrency : bool\n",
        "        If True, use `handle_request_concurrent` (with input concurrency).\n",
        "        If False, use `handle_request_scaled` (pure autoscaling focus).\n",
        "    num_requests : int\n",
        "        How many simulated API calls to send to Modal.\n",
        "\n",
        "    Usage examples (from a shell/terminal):\n",
        "    ---------------------------------------\n",
        "    - Pure scaling experiment:\n",
        "        modal run lesson2_scaling.py --num-requests 200\n",
        "\n",
        "    - Concurrency experiment:\n",
        "        modal run lesson2_scaling.py --use-concurrency --num-requests 500\n",
        "    \"\"\"\n",
        "    import time as _time\n",
        "    import random as _random\n",
        "\n",
        "    print(\"\\n===============================================\")\n",
        "    print(\"Lesson 2 â€“ Scaling & Input Concurrency in Modal\")\n",
        "    print(\"===============================================\\n\")\n",
        "\n",
        "\n",
        "    fn = handle_request_concurrent if use_concurrency else handle_request_scaled\n",
        "    print(f\"Using function: {fn_label}\")\n",
        "    print(f\"Number of simulated API requests: {num_requests}\\n\")\n",
        "\n",
        "\n",
        "    # Build a batch of fake requests\n",
        "    request_ids = list(range(num_requests))\n",
        "    payload_sizes = [_random.randint(10, 80) for _ in range(num_requests)]  # KB\n",
        "\n",
        "    inputs = list(zip(request_ids, payload_sizes))\n",
        "    print(\"Submitting requests to Modal...\\n\")\n",
        "\n",
        "\n",
        "    start = _time.time()\n",
        "\n",
        "    # Submit each request as a separate remote call\n",
        "    results = []\n",
        "    for req_id, size in inputs:\n",
        "        results.append(fn.remote(req_id, size))\n",
        "\n",
        "    total_duration = round(_time.time() - start, 2)\n",
        "\n",
        "    print(f\"Completed {len(results)} requests in {total_duration} seconds.\")\n",
        "\n",
        "\n",
        "    # Show a small sample of results\n",
        "    print(\"\\nSample responses:\")\n",
        "    for r in results[:5]:\n",
        "        print(r)\n",
        "\n",
        "    print(\"\\nðŸ‘‰ Now open your Modal dashboard and observe:\")\n",
        "    print(\"   - How many containers are running at peak load\")\n",
        "    print(\"   - How quickly containers appear and disappear\")\n",
        "    print(\"   - Whether input concurrency changes the pattern\\n\")\n",
        "\n",
        "    print(\"ðŸ§ª Try changing:\")\n",
        "    print(\"   - max_containers / min_containers / scaledown_window in `handle_request_scaled`\")\n",
        "    print(\"   - max_inputs / target_inputs in `handle_request_concurrent`\")\n",
        "    print(\"   - num_requests and see how behavior shifts.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# Extra cell â€“ export Lesson 2 as lesson2_scaling.py\n",
        "# ==========================================\n",
        "# Run this cell once. It will (re)create a Python script\n",
        "# that you (and students) can run locally with:\n",
        "#\n",
        "#   modal run lesson2_scaling.py --num-requests 200\n",
        "#   modal run lesson2_scaling.py --use-concurrency True --num-requests 500\n",
        "#\n",
        "# The script contains:\n",
        "#   - simulate_api_request\n",
        "#   - handle_request_scaled  (scaling experiment)\n",
        "#   - handle_request_concurrent  (input concurrency experiment)\n",
        "#   - main() as the Modal local entrypoint\n",
        "\n",
        "%%writefile lesson2_scaling.py\n",
        "import time\n",
        "import random\n",
        "from typing import Dict\n",
        "\n",
        "import modal\n",
        "\n",
        "# App for this lesson\n",
        "app = modal.App(\"lesson2-scaling-and-concurrency\")\n",
        "\n",
        "\n",
        "def simulate_api_request(request_id: int, payload_kb: int, base_latency: float = 0.3) -> Dict:\n",
        "    \"\"\"\n",
        "    Simulate handling a small 'API-style' request.\n",
        "\n",
        "    This is NOT CPU heavy. It imitates:\n",
        "      - Waiting on IO (time.sleep)\n",
        "      - A tiny amount of CPU work (short loop)\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    request_id : int\n",
        "        Just an identifier so we can track which call this was.\n",
        "    payload_kb : int\n",
        "        Fake \"payload size\" in kilobytes, used to slightly vary the work.\n",
        "    base_latency : float\n",
        "        Minimum time (in seconds) that each request takes.\n",
        "    \"\"\"\n",
        "    # Simulate variable latency: base time plus up to 200 ms jitter\n",
        "    jitter = random.uniform(0.0, 0.2)\n",
        "    simulated_latency = base_latency + jitter\n",
        "\n",
        "    # Sleep to mimic waiting on a database / external API\n",
        "    time.sleep(simulated_latency)\n",
        "\n",
        "    # Light CPU work: tiny checksum based on payload size\n",
        "    pseudo_checksum = 0\n",
        "    for i in range(payload_kb * 10):\n",
        "        pseudo_checksum = (pseudo_checksum + i) % 97\n",
        "\n",
        "    return {\n",
        "        \"request_id\": request_id,\n",
        "        \"payload_kb\": payload_kb,\n",
        "        \"simulated_latency\": round(simulated_latency, 3),\n",
        "        \"checksum\": pseudo_checksum,\n",
        "    }\n",
        "\n",
        "\n",
        "# ==================================================\n",
        "# Function 1 â€“ Focus on container scaling behavior\n",
        "# ==================================================\n",
        "#\n",
        "# Start with everything = None to let Modal decide.\n",
        "# Then try setting your own values and compare.\n",
        "#\n",
        "@app.function(\n",
        "    max_containers=None,      # e.g. 2, 5, 20\n",
        "    min_containers=None,      # e.g. 2 or 5\n",
        "    scaledown_window=None,    # e.g. 30 or 60 seconds\n",
        ")\n",
        "def handle_request_scaled(request_id: int, payload_kb: int) -> Dict:\n",
        "    \"\"\"Handle a single 'API request' with configurable scaling.\"\"\"\n",
        "    return simulate_api_request(request_id=request_id, payload_kb=payload_kb)\n",
        "\n",
        "\n",
        "# ==================================================\n",
        "# Function 2 â€“ Enable input concurrency per container\n",
        "# ==================================================\n",
        "#\n",
        "# We start with a concrete default:\n",
        "#   max_inputs=50 (required)\n",
        "#   target_inputs=40 (soft target)\n",
        "#\n",
        "# Students can change these numbers and re-run.\n",
        "#\n",
        "@app.function()\n",
        "@modal.concurrent(\n",
        "    max_inputs=50,   # REQUIRED: try 20, 50, 100, 200\n",
        "    target_inputs=40 # OPTIONAL: soft target near max_inputs\n",
        ")\n",
        "def handle_request_concurrent(request_id: int, payload_kb: int) -> Dict:\n",
        "    \"\"\"Handle a single 'API request' with input concurrency enabled.\"\"\"\n",
        "    return simulate_api_request(request_id=request_id, payload_kb=payload_kb, base_latency=0.2)\n",
        "\n",
        "\n",
        "@app.local_entrypoint()\n",
        "def main(\n",
        "    use_concurrency: bool = False,\n",
        "    num_requests: int = 100,\n",
        "):\n",
        "    \"\"\"\n",
        "    Lesson 2 entrypoint.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    use_concurrency : bool\n",
        "        If True, use `handle_request_concurrent` (with input concurrency).\n",
        "        If False, use `handle_request_scaled` (pure autoscaling focus).\n",
        "    num_requests : int\n",
        "        How many simulated API calls to send to Modal.\n",
        "\n",
        "    Usage examples (from a shell/terminal)\n",
        "    --------------------------------------\n",
        "    # Pure scaling experiment:\n",
        "    modal run lesson2_scaling.py --num-requests 200\n",
        "\n",
        "    # Concurrency experiment:\n",
        "    modal run lesson2_scaling.py --use-concurrency --num-requests 500\n",
        "    \"\"\"\n",
        "    import random as _random\n",
        "    import time as _time\n",
        "\n",
        "    print(\"\\n===============================================\")\n",
        "    print(\"Lesson 2 â€“ Scaling & Input Concurrency in Modal\")\n",
        "    print(\"===============================================\\n\")\n",
        "\n",
        "    # Decide which Modal function to use\n",
        "    if use_concurrency:\n",
        "        fn = handle_request_concurrent\n",
        "        fn_label = \"handle_request_concurrent (with @modal.concurrent)\"\n",
        "    else:\n",
        "        fn = handle_request_scaled\n",
        "        fn_label = \"handle_request_scaled (autoscaling only)\"\n",
        "\n",
        "    print(f\"Using function: {fn_label}\")\n",
        "    print(f\"Number of simulated API requests: {num_requests}\\n\")\n",
        "\n",
        "    # Build a batch of fake requests\n",
        "    request_ids = list(range(num_requests))\n",
        "    payload_sizes = [_random.randint(10, 80) for _ in range(num_requests)]  # KB\n",
        "    inputs = list(zip(request_ids, payload_sizes))\n",
        "\n",
        "    print(\"Submitting requests to Modal...\\n\")\n",
        "\n",
        "    start = _time.time()\n",
        "\n",
        "    # Submit each request as a separate remote call\n",
        "    results = []\n",
        "    for req_id, size in inputs:\n",
        "        results.append(fn.remote(req_id, size))\n",
        "\n",
        "    total_duration = round(_time.time() - start, 2)\n",
        "    print(f\"Completed {len(results)} requests in {total_duration} seconds.\")\n",
        "\n",
        "    # Show a small sample of results\n",
        "    print(\"\\nSample responses:\")\n",
        "    for r in results[:5]:\n",
        "        print(r)\n",
        "\n",
        "    print(\"\\nðŸ‘‰ Now open your Modal dashboard and observe:\")\n",
        "    print(\"   - How many containers are running at peak load\")\n",
        "    print(\"   - How quickly containers appear and disappear\")\n",
        "    print(\"   - Whether input concurrency changes the pattern\\n\")\n",
        "\n",
        "    print(\"ðŸ§ª Try changing:\")\n",
        "    print(\"   - max_containers / min_containers / scaledown_window in `handle_request_scaled`\")\n",
        "    print(\"   - max_inputs / target_inputs in `handle_request_concurrent`\")\n",
        "    print(\"   - num_requests and see how behavior shifts.\")"
      ],
      "metadata": {
        "id": "CPy8FtFfsg3o"
      },
      "id": "CPy8FtFfsg3o",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!modal run lesson2_scaling.py --num-requests 200"
      ],
      "metadata": {
        "id": "5EIat63Zsrfl"
      },
      "id": "5EIat63Zsrfl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!modal run lesson2_scaling.py --use-concurrency --num-requests 500"
      ],
      "metadata": {
        "id": "vM_JTtmls51r"
      },
      "id": "vM_JTtmls51r",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}